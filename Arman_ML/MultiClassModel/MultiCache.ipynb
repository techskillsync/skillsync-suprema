{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating MultiClass Cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "def results_to_cache(results_folder:str):\n",
    "\ti = 0\n",
    "\twhile os.path.exists(os.path.join(results_folder, f'{i}_result.jsonl')):\n",
    "\t\twith open(os.path.join(results_folder, f'{i}_result.jsonl')) as file:\n",
    "\t\t\tfor line in file:\n",
    "\t\t\t\t# Parsing the JSON string into a dict and appending to the list of results\n",
    "\t\t\t\tresponse_json = json.loads(line.strip())\n",
    "\t\t\t\tkey = response_json['custom_id']\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tcontent_json = json.loads(response_json['response']['body']['choices'][0]['message']['content'])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(f\"invalid content, skipping key {key}\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tcategory = content_json['categorization'].lower().strip()\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tprint(f\"{category} is an invalid response, skipping {key}\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\tif (category == \"not qualified\"):\n",
    "\t\t\t\t\tdataset[key] = 0\n",
    "\t\t\t\telif (category == \"minimally qualified\"):\n",
    "\t\t\t\t\tdataset[key] = 1\n",
    "\t\t\t\telif (category == \"somewhat qualified\"):\n",
    "\t\t\t\t\tdataset[key] = 2\n",
    "\t\t\t\telif (category == \"qualified\"):\n",
    "\t\t\t\t\tdataset[key] = 3\n",
    "\t\t\t\telif (category == \"very qualified\"):\n",
    "\t\t\t\t\tdataset[key] = 4\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"{category} is not a category, skipping {key}\")\n",
    "\t\ti+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not qualifieds is not a category, skipping 5034ab688d21c719f27fa61a9711af0b\n",
      "invalid content, skipping key 060c1d2727d9a6a9580b6af545395a8f\n",
      ":qualified dec.-::: is not a category, skipping 41db188d89bc3a735a29ff817a9a3d4f\n",
      "not qualified is an invalid response, skipping c0d777fa768fcc12e2e44c82fa99e1c6\n",
      "invalid content, skipping key 4d0a35020067a52f8bc274dfa1768576\n",
      "not qualified is an invalid response, skipping 2a602f8b44c09cbf5816aa51d8b70e2c\n",
      "invalid content, skipping key 21881585da195d22a7a51dbd590e4e10\n",
      "invalid content, skipping key 823df8797d7a40d2378db4ef7217ef38\n",
      "invalid content, skipping key 7d6723221d8eee3d51d9f41522e35845\n",
      "very is not a category, skipping 7cf1d1aea97d4cc9b6773ee9d8723be4\n",
      "invalid content, skipping key 3c3c966726e6cb3a33926eaf15cf9c94\n"
     ]
    }
   ],
   "source": [
    "# Process each request:\n",
    "RESULTS_FOLDER = \"../100K_Model/10K_AutoData\" # missing 12 records\n",
    "results_to_cache(RESULTS_FOLDER)\n",
    "# print(len(dataset))\n",
    "RESULTS_FOLDER = \"../100K_Model/3.6K_AutoData\" # missing 0 records\n",
    "results_to_cache(RESULTS_FOLDER)\n",
    "# print(len(dataset))\n",
    "RESULTS_FOLDER = \"../100K_Model/864K_AutoData\" # missing 100 records\n",
    "results_to_cache(RESULTS_FOLDER)\n",
    "# print(len(dataset))\n",
    "\n",
    "# Manually add all the cases that failed:\n",
    "dataset.update({'5034ab688d21c719f27fa61a9711af0b': 0 }) # not qualified\n",
    "dataset.update({'060c1d2727d9a6a9580b6af545395a8f': 0 }) # not qualified\n",
    "dataset.update({'41db188d89bc3a735a29ff817a9a3d4f': 3 }) # qualified\n",
    "dataset.update({'c0d777fa768fcc12e2e44c82fa99e1c6': 0 }) # not qualified\n",
    "dataset.update({'4d0a35020067a52f8bc274dfa1768576': 0 }) # not qualified\n",
    "dataset.update({'2a602f8b44c09cbf5816aa51d8b70e2c': 0 }) # not qualified\n",
    "dataset.update({'3c3c966726e6cb3a33926eaf15cf9c94': 0 }) # not qualified\n",
    "dataset.update({'7cf1d1aea97d4cc9b6773ee9d8723be4': 0 }) # not qualified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '100K_multi_cache.json'\n",
    "with open(output_file, 'w') as file:\n",
    "\tjson.dump(dataset, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".arman_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The links to the dataset can be found here:\n",
    "# https://www.kaggle.com/datasets/jillanisofttech/updated-resume-dataset\n",
    "# https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
    "# https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
    "\n",
    "# I combined the resume-dataset and updated-resume-dataset into resumes.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 484.30 MiB, increment: 0.53 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ssl\n",
    "import nltk\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"sk-proj-sScFMZr4GK3XaQ5mcYFYT3BlbkFJ1orIjm1gKSNT0q30KBMZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "postings = pd.read_csv(\"data/trunc-postings.csv\")\n",
    "resumes = pd.read_csv(\"data/trunc-resumes.csv\")\n",
    "postings = postings.head(3)\n",
    "resumes = resumes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text_series):\n",
    "    logging.info(\"Preprocessing text...\")\n",
    "    text_series = text_series.fillna(\"\")  # Replace NaN with empty strings\n",
    "    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_series = text_series.apply(lambda x: ' '.join(\n",
    "        lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words\n",
    "    ))\n",
    "    return text_series\n",
    "\n",
    "resumes['Resume'] = preprocess_text(resumes['Resume'])\n",
    "postings['description'] = preprocess_text(postings['description'])\n",
    "\n",
    "display(resumes.head(3))\n",
    "display(postings.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "universal_sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def vectorize_text(text_series):\n",
    "    logging.info(\"Vectorizing text...\")\n",
    "    embeddings = universal_sentence_encoder(text_series.tolist()).numpy()\n",
    "    return embeddings\n",
    "\n",
    "vectorized_resumes = vectorize_text(resumes['Resume'])\n",
    "vectorized_postings = vectorize_text(postings['description'])\n",
    "\n",
    "print(f'vectorized resumes: {type(vectorized_resumes)}, {vectorized_resumes.nbytes}')\n",
    "print(f'vectorized postings: {type(vectorized_postings)}, {vectorized_postings.nbytes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "resume_posting_pairs = [(vectorized_resumes[i], vectorized_postings[j])\n",
    "                for i in range(len(resumes)) for j in range(len(postings))]\n",
    "# Ensure vector pairs are numpy arrays\n",
    "resume_posting_pairs = [(np.array(left), np.array(right)) for left, right in resume_posting_pairs]\n",
    "\n",
    "print(f'resume_posting_pairs: {type(resume_posting_pairs)}, {len(resume_posting_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "if os.path.exists('cache.json'):\n",
    "    with open('cache.json', 'r') as cache_file:\n",
    "        cache = json.load(cache_file)\n",
    "else:\n",
    "    cache = {}\n",
    "    with open('cache.json', 'w') as cache_file:\n",
    "        json.dump(cache, cache_file)\n",
    "\n",
    "def generate_labels(descriptions, resumes):\n",
    "    logging.info(\"Generating labels...\")\n",
    "    labels = []\n",
    "\n",
    "    for i, description in enumerate(descriptions):\n",
    "        for j, resume in enumerate(resumes):\n",
    "            key = hashlib.md5(f\"{resume}{description}\".encode()).hexdigest()\n",
    "\n",
    "            if key in cache:\n",
    "                relevance = cache[key]\n",
    "            else:\n",
    "                prompt = f\"How relevant is this resume to the job description? {resume} {description}\"\n",
    "\n",
    "                try:\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"Analyze the relevance of the resume to the job description.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ]\n",
    "                    )\n",
    "                    relevance_text = response.choices[0].message.content.strip()\n",
    "                    relevance = 1 if any(phrase in relevance_text for phrase in ['highly relevant', 'very relevant', 'extremely relevant']) else 0\n",
    "                    cache[key] = relevance\n",
    "                except Exception as e:\n",
    "                    print(f'Error {e}')\n",
    "\n",
    "            labels.append(relevance)\n",
    "\n",
    "    with open('cache.json', 'w') as cache_file:\n",
    "        json.dump(cache, cache_file)\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels = generate_labels(postings['description'], resumes['Resume'])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model (needs nvidia driver)\n",
    "device = torch.device('cuda'))\n",
    "device = torch.device('cuda')\n",
    "train_pairs, test_pairs, train_labels, test_labels = train_test_split(resume_posting_pairs, labels, test_size=0.2, random_state=42)\n",
    "logging.info(\"Data split into training and testing sets.\")\n",
    "print(\"Training and testing data prepared.\")\n",
    "\n",
    "train_left = np.array([x[0] for x in train_pairs])\n",
    "train_right = np.array([x[1] for x in train_pairs])\n",
    "test_left = np.array([x[0] for x in test_pairs])\n",
    "test_right = np.array([x[1] for x in test_pairs])\n",
    "\n",
    "train_data = (torch.tensor(train_left, dtype=torch.float32), torch.tensor(train_right, dtype=torch.float32))\n",
    "test_data = (torch.tensor(test_left, dtype=torch.float32), torch.tensor(test_right, dtype=torch.float32))\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"Train Data Shape:\", (train_left.shape, train_right.shape))\n",
    "print(\"Train Data Type:\", (type(train_left), type(train_right)))\n",
    "print(\"Train Labels Shape:\", len(train_labels))\n",
    "print(\"Train Labels Type:\", type(train_labels))\n",
    "print(\"Test Data Shape:\", (test_left.shape, test_right.shape))\n",
    "print(\"Test Data Type:\", (type(test_left), type(test_right)))\n",
    "print(\"Test Labels Shape:\", len(test_labels))\n",
    "print(\"Test Labels Type:\", type(test_labels))\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.shared_network = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "    def forward_one_side(self, x):\n",
    "        return self.shared_network(x)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one_side(input1)\n",
    "        output2 = self.forward_one_side(input2)\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        output = torch.sigmoid(self.output_layer(distance))\n",
    "        return output\n",
    "\n",
    "input_size = train_left.shape[1]\n",
    "\n",
    "model = SiameseNetwork(input_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data[0], train_data[1], train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data[0], test_data[1], test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "logging.info(\"Starting model training...\")\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (left, right, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(left.to(device), right.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (left, right, labels) in enumerate(test_loader):\n",
    "            outputs = model(left.to(device), right.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'siamese_network.pth')\n",
    "        logging.info(\"Model saved.\")\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            logging.info(\"Early stopping triggered.\")\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "logging.info(\"Model training completed.\")\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "model.load_state_dict(torch.load('siamese_network.pth', map_location=device))\n",
    "logging.info(\"Best model loaded.\")\n",
    "print(\"Best model loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

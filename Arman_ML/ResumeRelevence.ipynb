{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The links to the dataset can be found here:\n",
    "# https://www.kaggle.com/datasets/jillanisofttech/updated-resume-dataset\n",
    "# https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
    "# https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\n",
    "\n",
    "# I combined the resume-dataset and updated-resume-dataset into resumes.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 397.77 MiB, increment: 0.47 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ssl\n",
    "import nltk\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"sk-proj-sScFMZr4GK3XaQ5mcYFYT3BlbkFJ1orIjm1gKSNT0q30KBMZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 413.89 MiB, increment: 13.95 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "postings = pd.read_csv(\"data/trunc-postings.csv\")\n",
    "resumes = pd.read_csv(\"data/trunc-resumes.csv\")\n",
    "postings = postings.head(3)\n",
    "resumes = resumes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "peak memory: 396.31 MiB, increment: 1.70 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "if os.path.exists('cache.json'):\n",
    "\twith open('cache.json', 'r') as cache_file:\n",
    "\t\tcache = json.load(cache_file)\n",
    "else:\n",
    "\tprint(\"ERROR: Could not find cache. Aborting\")\n",
    "\tsys.exit(1)\n",
    "\n",
    "\n",
    "labels = []\n",
    "for job_description in postings['description']:\n",
    "\tfor resume_text in resumes['Resume']:\n",
    "\t\tkey = hashlib.md5(f\"{resume_text}{job_description}\".encode()).hexdigest()\n",
    "\t\trelevance = cache.get(key)\n",
    "\t\tif relevance == None:\n",
    "\t\t\tprint(f\"ERROR: COULD NOT FIND LABEL FOR KEY {key}. THIS SHOULD BE ADDRESSED. DEFAULTING TO 0\")\n",
    "\t\t\trelevance = 0\n",
    "\t\tlabels.append(relevance)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/armandrismir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/armandrismir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hr administratormarketing associate hr adminis...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hr specialist u hr operation summary versatile...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hr director summary 20 year experience recruit...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Resume Category\n",
       "0  hr administratormarketing associate hr adminis...       HR\n",
       "1  hr specialist u hr operation summary versatile...       HR\n",
       "2  hr director summary 20 year experience recruit...       HR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>job descriptiona leading real estate firm new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aspen therapy wellness committed serving clien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>national exemplar accepting application assist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description\n",
       "0  job descriptiona leading real estate firm new ...\n",
       "1  aspen therapy wellness committed serving clien...\n",
       "2  national exemplar accepting application assist..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 397.67 MiB, increment: 4.48 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text_series):\n",
    "    logging.info(\"Preprocessing text...\")\n",
    "    text_series = text_series.fillna(\"\")  # Replace NaN with empty strings\n",
    "    text_series = text_series.apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_series = text_series.apply(lambda x: ' '.join(\n",
    "        lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words\n",
    "    ))\n",
    "    return text_series\n",
    "\n",
    "resumes['Resume'] = preprocess_text(resumes['Resume'])\n",
    "postings['description'] = preprocess_text(postings['description'])\n",
    "\n",
    "display(resumes.head(3))\n",
    "display(postings.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized resumes: <class 'numpy.ndarray'>, 6144\n",
      "vectorized postings: <class 'numpy.ndarray'>, 6144\n",
      "peak memory: 875.00 MiB, increment: 477.66 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "universal_sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def vectorize_text(text_series):\n",
    "    logging.info(\"Vectorizing text...\")\n",
    "    embeddings = universal_sentence_encoder(text_series.tolist()).numpy()\n",
    "    return embeddings\n",
    "\n",
    "vectorized_resumes = vectorize_text(resumes['Resume'])\n",
    "vectorized_postings = vectorize_text(postings['description'])\n",
    "\n",
    "print(f'vectorized resumes: {type(vectorized_resumes)}, {vectorized_resumes.nbytes}')\n",
    "print(f'vectorized postings: {type(vectorized_postings)}, {vectorized_postings.nbytes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_posting_pairs: <class 'list'>, 9\n",
      "peak memory: 527.52 MiB, increment: -2.62 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "resume_posting_pairs = [(vectorized_resumes[i], vectorized_postings[j])\n",
    "                for i in range(len(resumes)) for j in range(len(postings))]\n",
    "# Ensure vector pairs are numpy arrays\n",
    "resume_posting_pairs = [(np.array(left), np.array(right)) for left, right in resume_posting_pairs]\n",
    "\n",
    "print(f'resume_posting_pairs: {type(resume_posting_pairs)}, {len(resume_posting_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model (needs nvidia driver)\n",
    "device = torch.device('cuda'))\n",
    "device = torch.device('cuda')\n",
    "train_pairs, test_pairs, train_labels, test_labels = train_test_split(resume_posting_pairs, labels, test_size=0.2, random_state=42)\n",
    "logging.info(\"Data split into training and testing sets.\")\n",
    "print(\"Training and testing data prepared.\")\n",
    "\n",
    "train_left = np.array([x[0] for x in train_pairs])\n",
    "train_right = np.array([x[1] for x in train_pairs])\n",
    "test_left = np.array([x[0] for x in test_pairs])\n",
    "test_right = np.array([x[1] for x in test_pairs])\n",
    "\n",
    "train_data = (torch.tensor(train_left, dtype=torch.float32), torch.tensor(train_right, dtype=torch.float32))\n",
    "test_data = (torch.tensor(test_left, dtype=torch.float32), torch.tensor(test_right, dtype=torch.float32))\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"Train Data Shape:\", (train_left.shape, train_right.shape))\n",
    "print(\"Train Data Type:\", (type(train_left), type(train_right)))\n",
    "print(\"Train Labels Shape:\", len(train_labels))\n",
    "print(\"Train Labels Type:\", type(train_labels))\n",
    "print(\"Test Data Shape:\", (test_left.shape, test_right.shape))\n",
    "print(\"Test Data Type:\", (type(test_left), type(test_right)))\n",
    "print(\"Test Labels Shape:\", len(test_labels))\n",
    "print(\"Test Labels Type:\", type(test_labels))\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.shared_network = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "\n",
    "    def forward_one_side(self, x):\n",
    "        return self.shared_network(x)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one_side(input1)\n",
    "        output2 = self.forward_one_side(input2)\n",
    "        distance = torch.abs(output1 - output2)\n",
    "        output = torch.sigmoid(self.output_layer(distance))\n",
    "        return output\n",
    "\n",
    "input_size = train_left.shape[1]\n",
    "\n",
    "model = SiameseNetwork(input_size).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_data[0], train_data[1], train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data[0], test_data[1], test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "logging.info(\"Starting model training...\")\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (left, right, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(left.to(device), right.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (left, right, labels) in enumerate(test_loader):\n",
    "            outputs = model(left.to(device), right.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    logging.info(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'siamese_network.pth')\n",
    "        logging.info(\"Model saved.\")\n",
    "        print(\"Model saved.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            logging.info(\"Early stopping triggered.\")\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "logging.info(\"Model training completed.\")\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "model.load_state_dict(torch.load('siamese_network.pth', map_location=device))\n",
    "logging.info(\"Best model loaded.\")\n",
    "print(\"Best model loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
